<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><meta name=author content="Md. Akram Hossain"><meta name=description content="Cause-effect relationships play a crucial role in human cognition, and distilling cause-effect relations from text helps in ameliorating causal networks for predictive tasks including natural language-based financial forecasting, text summarization, and question-answering. However, the lack of syntactic clues, the ambivalent semantic meaning of words, and complex sentence structures make it one of the challenging tasks in NLP. To address these challenges, CASE-2023 introduced a shared task 3 with two subtasks focusing on event causality identification with causal news corpus. In this paper, we demonstrate our participant systems for this task. We leverage two transformers models including DeBERTa and Twitter-RoBERTa along with the weighted average fusion technique to tackle the challenges of subtask 1 where we need to identify whether a text belongs to either causal or not. For subtask 2 where we need to identify the cause, effect, and signal tokens from the text, we proposed a unified neural network of DeBERTa and DistilRoBERTa transformer variants with contrastive learning techniques. The experimental results showed that our proposed method achieved competitive performance among the participants’ systems and achieved 4th and 3rd rank in subtasks 1 and 2 respectively."><link rel=alternate hreflang=en-gb href=https://cardiffnlp.github.io/publication/case-ranlp-conference-2023-cause-effect/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.1776d75f194aebe4673e71e97828f9f8.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu872eabe117abf289aa56f334b430b015_254222_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu872eabe117abf289aa56f334b430b015_254222_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://cardiffnlp.github.io/publication/case-ranlp-conference-2023-cause-effect/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@csecudsg"><meta property="twitter:creator" content="@csecudsg"><meta property="og:site_name" content="CSECU-DSG"><meta property="og:url" content="https://cardiffnlp.github.io/publication/case-ranlp-conference-2023-cause-effect/"><meta property="og:title" content="CSECU-DSG@ Causal News Corpus 2023: Leveraging RoBERTa and DeBERTa Transformer Model with Contrastive Learning for Causal Event Classification | CSECU-DSG"><meta property="og:description" content="Cause-effect relationships play a crucial role in human cognition, and distilling cause-effect relations from text helps in ameliorating causal networks for predictive tasks including natural language-based financial forecasting, text summarization, and question-answering. However, the lack of syntactic clues, the ambivalent semantic meaning of words, and complex sentence structures make it one of the challenging tasks in NLP. To address these challenges, CASE-2023 introduced a shared task 3 with two subtasks focusing on event causality identification with causal news corpus. In this paper, we demonstrate our participant systems for this task. We leverage two transformers models including DeBERTa and Twitter-RoBERTa along with the weighted average fusion technique to tackle the challenges of subtask 1 where we need to identify whether a text belongs to either causal or not. For subtask 2 where we need to identify the cause, effect, and signal tokens from the text, we proposed a unified neural network of DeBERTa and DistilRoBERTa transformer variants with contrastive learning techniques. The experimental results showed that our proposed method achieved competitive performance among the participants’ systems and achieved 4th and 3rd rank in subtasks 1 and 2 respectively."><meta property="og:image" content="https://cardiffnlp.github.io/media/logo_hu872eabe117abf289aa56f334b430b015_254222_300x300_fit_lanczos_3.png"><meta property="twitter:image" content="https://cardiffnlp.github.io/media/logo_hu872eabe117abf289aa56f334b430b015_254222_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-gb"><meta property="article:published_time" content="2023-09-07T22:41:36+00:00"><meta property="article:modified_time" content="2023-09-07T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://cardiffnlp.github.io/publication/case-ranlp-conference-2023-cause-effect/"},"headline":"CSECU-DSG@ Causal News Corpus 2023: Leveraging RoBERTa and DeBERTa Transformer Model with Contrastive Learning for Causal Event Classification","datePublished":"2023-09-07T22:41:36Z","dateModified":"2023-09-07T00:00:00Z","author":{"@type":"Person","name":"Md. Akram Hossain"},"publisher":{"@type":"Organization","name":"CSECU-DSG","logo":{"@type":"ImageObject","url":"https://cardiffnlp.github.io/media/logo_hu872eabe117abf289aa56f334b430b015_254222_192x192_fit_lanczos_3.png"}},"description":"Cause-effect relationships play a crucial role in human cognition, and distilling cause-effect relations from text helps in ameliorating causal networks for predictive tasks including natural language-based financial forecasting, text summarization, and question-answering. However, the lack of syntactic clues, the ambivalent semantic meaning of words, and complex sentence structures make it one of the challenging tasks in NLP. To address these challenges, CASE-2023 introduced a shared task 3 with two subtasks focusing on event causality identification with causal news corpus. In this paper, we demonstrate our participant systems for this task. We leverage two transformers models including DeBERTa and Twitter-RoBERTa along with the weighted average fusion technique to tackle the challenges of subtask 1 where we need to identify whether a text belongs to either causal or not. For subtask 2 where we need to identify the cause, effect, and signal tokens from the text, we proposed a unified neural network of DeBERTa and DistilRoBERTa transformer variants with contrastive learning techniques. The experimental results showed that our proposed method achieved competitive performance among the participants’ systems and achieved 4th and 3rd rank in subtasks 1 and 2 respectively."}</script><title>CSECU-DSG@ Causal News Corpus 2023: Leveraging RoBERTa and DeBERTa Transformer Model with Contrastive Learning for Causal Event Classification | CSECU-DSG</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=421ca8197f08f6cd7a91512dae3d615c><script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/media/logo_hu872eabe117abf289aa56f334b430b015_254222_0x70_resize_lanczos_3.png alt=CSECU-DSG></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/media/logo_hu872eabe117abf289aa56f334b430b015_254222_0x70_resize_lanczos_3.png alt=CSECU-DSG></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/post><span>News</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/research><span>Projects</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/gallery><span>Gallery</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>CSECU-DSG@ Causal News Corpus 2023: Leveraging RoBERTa and DeBERTa Transformer Model with Contrastive Learning for Causal Event Classification</h1><div class=article-metadata><div><span class=author-highlighted>Md. Akram Hossain</span>, <span class=author-highlighted>Abdul Aziz</span>, <span class=author-highlighted>Abu Nowshed Chy</span></div><span class=article-date>September 2023</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href=https://aclanthology.org/2023.case-1.15.pdf target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/case-ranlp-conference-2023-cause-effect/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header" href=https://doi.org/https://doi.org/10.26615/978-954-452-089-2_015 target=_blank rel=noopener>DOI</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Cause-effect relationships play a crucial role in human cognition, and distilling cause-effect relations from text helps in ameliorating causal networks for predictive tasks including natural language-based financial forecasting, text summarization, and question-answering. However, the lack of syntactic clues, the ambivalent semantic meaning of words, and complex sentence structures make it one of the challenging tasks in NLP. To address these challenges, CASE-2023 introduced a shared task 3 with two subtasks focusing on event causality identification with causal news corpus. In this paper, we demonstrate our participant systems for this task. We leverage two transformers models including DeBERTa and Twitter-RoBERTa along with the weighted average fusion technique to tackle the challenges of subtask 1 where we need to identify whether a text belongs to either causal or not. For subtask 2 where we need to identify the cause, effect, and signal tokens from the text, we proposed a unified neural network of DeBERTa and DistilRoBERTa transformer variants with contrastive learning techniques. The experimental results showed that our proposed method achieved competitive performance among the participants’ systems and achieved 4th and 3rd rank in subtasks 1 and 2 respectively.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#1>Conference paper</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">Proceedings of the 6th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE@RANLP 2023)</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=share-box><ul class=share></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/author/md.-akram-hossain/avatar_hu73fd685dccae9e01d36da6e0c5aa6fea_287129_270x270_fill_q75_lanczos_center.jpg alt="Md. Akram Hossain"><div class=media-body><h5 class=card-title>Md. Akram Hossain</h5><h6 class=card-subtitle>Research Assistant (Full Time)</h6><p class=card-text>check</p><ul class=network-icon aria-hidden=true><li><a href=https://akram1871.github.io/ target=_blank rel=noopener><i class="fas fa-globe"></i></a></li><li><a href="https://scholar.google.com/citations?user=VgYvKn0AAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://www.linkedin.com/in/md-akram-hossain-661195199/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/author/abdul-aziz/avatar_hu233b2de9089e00531ea9fa2706e44f89_255750_270x270_fill_q75_lanczos_center.jpg alt="Abdul Aziz"><div class=media-body><h5 class=card-title>Abdul Aziz</h5><h6 class=card-subtitle>Research Assistant (Full Time)</h6><ul class=network-icon aria-hidden=true><li><a href=https://azizcu.github.io/ target=_blank rel=noopener><i class="fas fa-globe"></i></a></li><li><a href="https://scholar.google.com/citations?user=SwoIfC0AAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://www.linkedin.com/in/abdul-aziz-9b67641a6/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/author/abu-nowshed-chy/avatar_huc5c5f7fefb29a42140adace70418fec9_24402_270x270_fill_q75_lanczos_center.jpg alt="Abu Nowshed Chy"><div class=media-body><h5 class=card-title>Abu Nowshed Chy</h5><h6 class=card-subtitle>Assistant Professor</h6><ul class=network-icon aria-hidden=true><li><a href="https://cu.ac.bd/public_profile/index.php?ein=5905" target=_blank rel=noopener><i class="fas fa-globe"></i></a></li><li><a href="https://scholar.google.com/citations?user=hQuUosgAAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2024 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script><script src=/en/js/wowchemy.min.236ae12851def447fde9370101ca79b4.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>