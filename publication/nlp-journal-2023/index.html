<!doctype html><html lang=en-gb><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><meta name=author content="Md. Akram Hossain"><meta name=description content="Lexical complexity prediction (LCP) determines the complexity level of words or phrases in a sentence. LCP has a significant impact on the enhancement of language translations, readability assessment, and text generation. However, the domain-specific technical word, the complex grammatical structure, the polysemy problem, the inter-word relationship, and dependencies make it challenging to determine the complexity of words or phrases. In this paper, we propose an integrated transformer regressor model named ITRM-LCP to estimate the lexical complexity of words and phrases where diverse contextual features are extracted from various transformer models. The transformer models are fine-tuned using the text-pair data. Then, a bidirectional LSTM-based regressor module is plugged on top of each transformer to learn the long-term dependencies and estimate the complexity scores. The predicted scores of each module are then aggregated to determine the final complexity score. We assess our proposed model using two benchmark datasets from shared tasks. Experimental findings demonstrate that our ITRM-LCP model obtains 10.2% and 8.2% improvement on the news and Wikipedia corpus of the CWI-2018 dataset, compared to the top-performing systems (DAT, CAMB, and TMU). Additionally, our ITRM-LCP model surpasses state-of-the-art LCP systems (DeepBlueAI, JUST-BLUE) by 1.5% and 1.34% for single and multi-word LCP tasks defined in the SemEval LCP-2021 task."><link rel=alternate hreflang=en-gb href=https://cardiffnlp.github.io/publication/nlp-journal-2023/><meta name=theme-color content="#1565c0"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css integrity crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=/css/wowchemy.1776d75f194aebe4673e71e97828f9f8.css><link rel=stylesheet href=/css/libs/chroma/github-light.min.css title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=/css/libs/chroma/dracula.min.css title=hl-dark media=print onload='this.media="all"' disabled><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huf9fbccc78191c11f012ac2550814c756_32514_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huf9fbccc78191c11f012ac2550814c756_32514_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://cardiffnlp.github.io/publication/nlp-journal-2023/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@csecudsg"><meta property="twitter:creator" content="@csecudsg"><meta property="og:site_name" content="CSECU-DSG"><meta property="og:url" content="https://cardiffnlp.github.io/publication/nlp-journal-2023/"><meta property="og:title" content="Leveraging contextual representations with BiLSTM-based regressor for lexical complexity prediction | CSECU-DSG"><meta property="og:description" content="Lexical complexity prediction (LCP) determines the complexity level of words or phrases in a sentence. LCP has a significant impact on the enhancement of language translations, readability assessment, and text generation. However, the domain-specific technical word, the complex grammatical structure, the polysemy problem, the inter-word relationship, and dependencies make it challenging to determine the complexity of words or phrases. In this paper, we propose an integrated transformer regressor model named ITRM-LCP to estimate the lexical complexity of words and phrases where diverse contextual features are extracted from various transformer models. The transformer models are fine-tuned using the text-pair data. Then, a bidirectional LSTM-based regressor module is plugged on top of each transformer to learn the long-term dependencies and estimate the complexity scores. The predicted scores of each module are then aggregated to determine the final complexity score. We assess our proposed model using two benchmark datasets from shared tasks. Experimental findings demonstrate that our ITRM-LCP model obtains 10.2% and 8.2% improvement on the news and Wikipedia corpus of the CWI-2018 dataset, compared to the top-performing systems (DAT, CAMB, and TMU). Additionally, our ITRM-LCP model surpasses state-of-the-art LCP systems (DeepBlueAI, JUST-BLUE) by 1.5% and 1.34% for single and multi-word LCP tasks defined in the SemEval LCP-2021 task."><meta property="og:image" content="https://cardiffnlp.github.io/media/logo_huf9fbccc78191c11f012ac2550814c756_32514_300x300_fit_lanczos_3.png"><meta property="twitter:image" content="https://cardiffnlp.github.io/media/logo_huf9fbccc78191c11f012ac2550814c756_32514_300x300_fit_lanczos_3.png"><meta property="og:locale" content="en-gb"><meta property="article:published_time" content="2023-11-03T23:41:36+00:00"><meta property="article:modified_time" content="2023-11-03T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://cardiffnlp.github.io/publication/nlp-journal-2023/"},"headline":"Leveraging contextual representations with BiLSTM-based regressor for lexical complexity prediction","datePublished":"2023-11-03T23:41:36Z","dateModified":"2023-11-03T00:00:00Z","author":{"@type":"Person","name":"Abdul Aziz"},"publisher":{"@type":"Organization","name":"CSECU-DSG","logo":{"@type":"ImageObject","url":"https://cardiffnlp.github.io/media/logo_huf9fbccc78191c11f012ac2550814c756_32514_192x192_fit_lanczos_3.png"}},"description":"Lexical complexity prediction (LCP) determines the complexity level of words or phrases in a sentence. LCP has a significant impact on the enhancement of language translations, readability assessment, and text generation. However, the domain-specific technical word, the complex grammatical structure, the polysemy problem, the inter-word relationship, and dependencies make it challenging to determine the complexity of words or phrases. In this paper, we propose an integrated transformer regressor model named ITRM-LCP to estimate the lexical complexity of words and phrases where diverse contextual features are extracted from various transformer models. The transformer models are fine-tuned using the text-pair data. Then, a bidirectional LSTM-based regressor module is plugged on top of each transformer to learn the long-term dependencies and estimate the complexity scores. The predicted scores of each module are then aggregated to determine the final complexity score. We assess our proposed model using two benchmark datasets from shared tasks. Experimental findings demonstrate that our ITRM-LCP model obtains 10.2% and 8.2% improvement on the news and Wikipedia corpus of the CWI-2018 dataset, compared to the top-performing systems (DAT, CAMB, and TMU). Additionally, our ITRM-LCP model surpasses state-of-the-art LCP systems (DeepBlueAI, JUST-BLUE) by 1.5% and 1.34% for single and multi-word LCP tasks defined in the SemEval LCP-2021 task."}</script><title>Leveraging contextual representations with BiLSTM-based regressor for lexical complexity prediction | CSECU-DSG</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=01bd5afbbe788525ca6b333720ac0cf7><script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/><img src=/media/logo_huf9fbccc78191c11f012ac2550814c756_32514_0x70_resize_lanczos_3.png alt=CSECU-DSG></a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/><img src=/media/logo_huf9fbccc78191c11f012ac2550814c756_32514_0x70_resize_lanczos_3.png alt=CSECU-DSG></a></div><div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/post><span>News</span></a></li><li class=nav-item><a class=nav-link href=/people><span>People</span></a></li><li class=nav-item><a class=nav-link href=/research><span>Projects</span></a></li><li class=nav-item><a class="nav-link active" href=/publication><span>Publications</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"></ul></div></nav></header></div><div class=page-body><div class=pub><div class="article-container pt-3"><h1>Leveraging contextual representations with BiLSTM-based regressor for lexical complexity prediction</h1><div class=article-metadata><div><span class=author-highlighted>Abdul Aziz</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Md. Akram Hossain</span><i class="author-notes fas fa-info-circle" data-toggle=tooltip title="Equal contribution"></i>, <span class=author-highlighted>Abu Nowshed Chy</span>, <span>Md Zia Ullah</span>, <span>Masaki Aono</span></div><span class=article-date>November 2023</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href="https://www.sciencedirect.com/science/article/pii/S2949719123000365/pdfft?md5=502faaa940a69ced559675ed79aff467&amp;pid=1-s2.0-S2949719123000365-main.pdf" target=_blank rel=noopener>PDF
</a><a href=# class="btn btn-outline-primary btn-page-header js-cite-modal" data-filename=/publication/nlp-journal-2023/cite.bib>Cite
</a><a class="btn btn-outline-primary btn-page-header" href=https://doi.org/https://doi.org/10.1016/j.nlp.2023.100039 target=_blank rel=noopener>DOI</a></div></div><div class=article-container><h3>Abstract</h3><p class=pub-abstract>Lexical complexity prediction (LCP) determines the complexity level of words or phrases in a sentence. LCP has a significant impact on the enhancement of language translations, readability assessment, and text generation. However, the domain-specific technical word, the complex grammatical structure, the polysemy problem, the inter-word relationship, and dependencies make it challenging to determine the complexity of words or phrases. In this paper, we propose an integrated transformer regressor model named ITRM-LCP to estimate the lexical complexity of words and phrases where diverse contextual features are extracted from various transformer models. The transformer models are fine-tuned using the text-pair data. Then, a bidirectional LSTM-based regressor module is plugged on top of each transformer to learn the long-term dependencies and estimate the complexity scores. The predicted scores of each module are then aggregated to determine the final complexity score. We assess our proposed model using two benchmark datasets from shared tasks. Experimental findings demonstrate that our ITRM-LCP model obtains 10.2% and 8.2% improvement on the news and Wikipedia corpus of the CWI-2018 dataset, compared to the top-performing systems (DAT, CAMB, and TMU). Additionally, our ITRM-LCP model surpasses state-of-the-art LCP systems (DeepBlueAI, JUST-BLUE) by 1.5% and 1.34% for single and multi-word LCP tasks defined in the SemEval LCP-2021 task.</p><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Type</div><div class="col-12 col-md-9"><a href=/publication/#2>Journal article</a></div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=row><div class=col-md-1></div><div class=col-md-10><div class=row><div class="col-12 col-md-3 pub-row-heading">Publication</div><div class="col-12 col-md-9">Elsevier Natural Language Processing Journal, Vol. 05, No. 100039, 2023</div></div></div><div class=col-md-1></div></div><div class="d-md-none space-below"></div><div class=space-below></div><div class=article-style></div><div class=share-box><ul class=share></ul></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/author/abdul-aziz/avatar_hu233b2de9089e00531ea9fa2706e44f89_255750_270x270_fill_q75_lanczos_center.jpg alt="Abdul Aziz"><div class=media-body><h5 class=card-title>Abdul Aziz</h5><h6 class=card-subtitle>Research Assistant (Full Time)</h6><ul class=network-icon aria-hidden=true><li><a href=https://azizcu.github.io/ target=_blank rel=noopener><i class="fas fa-globe"></i></a></li><li><a href="https://scholar.google.com/citations?user=SwoIfC0AAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://www.linkedin.com/in/abdul-aziz-9b67641a6/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/author/md.-akram-hossain/avatar_hu73fd685dccae9e01d36da6e0c5aa6fea_287129_270x270_fill_q75_lanczos_center.jpg alt="Md. Akram Hossain"><div class=media-body><h5 class=card-title>Md. Akram Hossain</h5><h6 class=card-subtitle>Research Assistant (Full Time)</h6><ul class=network-icon aria-hidden=true><li><a href=https://akram1871.github.io/ target=_blank rel=noopener><i class="fas fa-globe"></i></a></li><li><a href="https://scholar.google.com/citations?user=VgYvKn0AAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://www.linkedin.com/in/md-akram-hossain-661195199/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src=/author/abu-nowshed-chy/avatar_huc5c5f7fefb29a42140adace70418fec9_24402_270x270_fill_q75_lanczos_center.jpg alt="Abu Nowshed Chy"><div class=media-body><h5 class=card-title>Abu Nowshed Chy</h5><h6 class=card-subtitle>Assistant Professor</h6><ul class=network-icon aria-hidden=true><li><a href="https://cu.ac.bd/public_profile/index.php?ein=5905" target=_blank rel=noopener><i class="fas fa-globe"></i></a></li><li><a href="https://scholar.google.com/citations?user=hQuUosgAAAAJ&amp;hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li></ul></div></div></div></div></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2023 Me. This work is licensed under <a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank>CC BY NC ND 4.0</a></p><p class="powered-by footer-license-icons"><a href=https://creativecommons.org/licenses/by-nc-nd/4.0 rel="noopener noreferrer" target=_blank aria-label="Creative Commons"><i class="fab fa-creative-commons fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-by fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nc fa-2x" aria-hidden=true></i>
<i class="fab fa-creative-commons-nd fa-2x" aria-hidden=true></i></a></p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.b4708d4364577c16ab7001b265a063a4.js></script><script src=https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js integrity crossorigin=anonymous></script><script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script><script src=/en/js/wowchemy.min.236ae12851def447fde9370101ca79b4.js></script><script src=/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js type=module></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy
</a><a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>